# README FILE FOR DATABYTE TASK 1 ( WINE QUALITY PREDICTION).

Wine Dataset available on UCI Repository is of two categories:

1. Red Wine Dataset and

1. White Wine Dataset.

For DataByte Induction Task We will be working on Red Wine Dataset to explore the Features upon which the quality of the Red Wine depends.

THE FLOW OF THE README FILE WILL BE AS FOLLOWS:

1. INSIGHTS OF THE EDA PERFORMED ON THE DATASET.

1. NAIVE BAYES ALGORITHM FROM SCRATCH.

1. DECISION TREE ALGORITHM FROM SCRATCH.

1. BASIC IMPLEMENTATION OF ANN FROM SCRATCH.

1. IMPLEMENTATION OF ANN USING TENSORLOW AND KERAS.

## INSIGHTS OF THE EDA PERFORMED

From the EXPLORATORY DATA ANALYSIS Performed on the Dataset, we get to know the very accurate analysis of the Data.

1. SHAPE OF THE DATA:

'''  
(1599, 12)
'''
  ### So there are 1599 Data points in the dataset, each described by 12 features including the label.

2. FEATURES IN THE DATASET:
   

DETERMINING THE NULL VALUES AND DATA TYPES:

  #   Column                Non-Null Count  Dtype  
  ---  ------                --------------  -----  
   0   fixed acidity         1599 non-null   float64
   1   volatile acidity      1599 non-null   float64
   2   citric acid           1599 non-null   float64
   3   residual sugar        1599 non-null   float64
   4   chlorides             1599 non-null   float64
   5   free sulfur dioxide   1599 non-null   int64  
   6   total sulfur dioxide  1599 non-null   int64  
   7   density               1599 non-null   float64
   8   pH                    1599 non-null   float64
   9   sulphates             1599 non-null   float64
   10  alcohol               1599 non-null   float64
   11  quality               1599 non-null   int64  
  dtypes: float64(9), int64(3)

STATISTICAL INSIGHTS:

array([3, 4, 5, 6, 7, 8], dtype=int64)

CLASS IMBALANCE:

BY LOOKING AT THE GRAPHS WE CAN FIND THE SPREAD OF THE FEATURES AND ALSO WE GET TO KNOW THAT FEATURES HAVE OUTLIERS AS WILL BE PROVED FROM THE BOX PLOT OF EACH FEATURE AS

MATHEMATICAL INTERPRETATION OF OUTLIERS(Z-SCORES: Z-score is a way to standardize the data to a standard scale i.e. how far the data point is from the mean.)

NUMBER OF OUTLIERS PER CLASS.

THEN WE GO FOR BIVARIATE ANALYSIS FOR FINDING THE DEPENDENCIES BETWEEN THE FEATURES OF THE DATASET, BY PLOTTING PAIR-WISE PLOT. BY THIS WE GOT TO KNOW THAT THERE ARE ONLY A FEW FEATURES WIHIC IS RELATED THIS IS THE CAUSE FOR WHICH THE NAIVE BAYES MODEL OUTPERFORM THE DECISION TREE MODEL.

THROUGH ALL THIS WE GOT TO KNOW:

THE DATASET HAS 1599 POINTS AND 12 FEATURES

6 CLASS LABELS

DATASET IS ALSO IMBALANCED

DATASET CONTAINS APPROX 11 PERCENT OUTLIERS.

THERE IS VERY LESS CORRELATION BETWEEN THE FEATURES.

THE CHLORIDE CLASS HAS MAXIMUM OUTLIERS.

CITRIC ACID HAS MINIMUM OUTLIERS.

AS THE ALCOHOL CONTENT INCREASES QUALITY INCREASES.

VOLATILE ACIDITY HAS THE MAXIMUM NEGATIVE IMPACT ON THE WINE QUALITY.

FREE SULPHUR CONTENT HAS THE LEAST IMPACT ON THE WINE QUALITY.

ANALYSIS OF DIFFERENT MODELS IMPLEMENTED ON THE DATASET:

NAIVE BAYES:

ACCURACY: 61.25%

CONFUSION MATRIX:

BY CONFUSION MATRIX WE GET TO KNOW THAT CLASS 5 AND 6 ARE THE ONES FOR ACCURACY IS LOW BUT THEY HAVE HIGHER PRECISION i.e. WHEN OUR MODEL PREDICT SOME POINT AS 5 THEN MORE THAN 60% SURE THAT IT IS CORRECT AND ALSO WHEN OUR MODEL PREDICT SOMR POINT AS 5 OUT ALL THE CORRECT CLASSES THEN IT MORE THAN 65% SURE THAT POINTS PREDICTED AS 5 ARE CORRECT. SIMILARLY, F1_SCORE IS JUST THE HARMONIC MEAN OF THE RECALL AND PRECISION.

THE FOLLOWING GRAPHS ILLUSTRATE THE EVALUATION OF THE MODEL ON DIFFERENT PARAMETERS CONCERNING CLASS LABELS.

CLASSWISE ACCURACY

CLASSWISE PRECISION

CLASSWISE RECALL

CLASSWISE F1_SCORE

DECISION TREE

ACCURACY: 44.6875%

CONFUSION MATRIX:

CLASSWISE ACCURACY:

CLASSWISE PRECISION:

CLASSWISE RECALL

CLASSWISE F1_SCORE

INSIGHTS FROM THE GRAPHS :

DUE TO CLASS IMBALANCE, THERE IS THERE IS NOT MUCH CHANGE IN THE METRIC EVALUATION OF CLASSES EXCEPT 5 AND 6 WHICH ARE THE MOST DENSIFIED CLASSES IN THE DATASET. SO SLIGHT CHANGE IN THE PREDICTION OF THESE IMPACTS THE ACCURACY OF THE MODEL.

DECISION TREE PREDICTIONS ARE SIMILAR TO NAIVE BAYES EXCEPT IT GIVES MORE ACCURACY FOR 5 THAN THE FORMER MODEL.

ARTIFICIAL NEURAL NETWORKS FROM SCRATCH AND WITH TENSORFLOW AND KERAS:

ACCURACY: 54.06% AFTER 100 EPOCHS OF TRAINING.

CONCLUSION:

OUT OF ALL THE MODELS IMPLEMENTED NAIVE BAYES GIVE THE MAXIMUM ACCURACY. THIS CAN BE DUE TO THE FOLLOWING PARAMETER:

OUTLIERS: AS NAIVE BAYES ASSUMES VERY LESS CORRELATION BETWEEN THE FEATURES THUS EFFECT OF OUTLIERS IS VERY LESS.

CLASSIMBALANCE: THE EFFECT OF CLASSIMBALANCE IS LESS AS IT OPERATES ON THE PRINCIPLE OF PROBABILITY DISTRIBUTION.

Also, ANN AND DECISION TREE ARE MUCH MORE COMPLEX ALGORITHMS AS COMPARED TO NAIVE BAYES. THUS THERE IS A CHANCE THAT THESE MODELS MAY REQUIRE MORE DATASETS THAN PROVIDED WHICH IS JUST 1599 DATAPOINT WITH 11% OUTLIERS AND A LARGE AMOUNT OF CLASS IMBALANCE.
